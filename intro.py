import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
import os
import io
import base64

# Set page configuration
st.set_page_config(
    page_title="WQI Prediction App",
    page_icon="ðŸ’§",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Add CSS for styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #0078ff;
        text-align: center;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0066cc;
    }
    .metric-card {
        background-color: #f0f8ff;
        padding: 15px;
        border-radius: 5px;
        box-shadow: 2px 2px 5px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# Functions
@st.cache_data
def load_data_from_local(file_path):
    """
    Load data from a local file path
    """
    try:
        # Check if file exists
        if not os.path.isfile(file_path):
            return None, f"Error: File not found at {file_path}"
            
        # Determine file type and load accordingly
        if file_path.endswith('.csv'):
            data = pd.read_csv(file_path)
        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            data = pd.read_excel(file_path)
        else:
            return None, "Error: File must be CSV or Excel format"
            
        return data, f"Successfully loaded {data.shape[0]} rows from {file_path}"
    except Exception as e:
        return None, f"Error loading data: {str(e)}"

@st.cache_data
def preprocess_data(data):
    """
    Apply the same preprocessing steps as during model training
    """
    try:
        # Step 1: Drop unnecessary columns
        data = data.drop(columns=['WaterbodyName', 'Years', 'SampleDate', 'Label'], errors='ignore')
        
        # Step 2: Create a copy for outlier removal
        df = data.copy()
        
        # Step 3: Remove outliers using IQR method
        for col in data.select_dtypes(include=np.number):
            # Calculate the 25th and 75th percentiles (Q1 and Q3)
            percentile25 = df[col].quantile(0.25)
            percentile75 = df[col].quantile(0.75)

            # Calculate the IQR
            iqr = percentile75 - percentile25

            # Set the lower and upper bounds for outliers
            lower_limit = percentile25 - 1.5 * iqr
            upper_limit = percentile75 + 1.5 * iqr

            # Remove outliers by filtering the DataFrame
            df = df[(df[col] >= lower_limit) & (df[col] <= upper_limit)]
        
        # Step 4: Normalize data using MinMaxScaler
        scaler = MinMaxScaler()
        features_to_scale = df.columns[df.columns != 'WQI Value']
        df[features_to_scale] = scaler.fit_transform(df[features_to_scale])
        
        return df, f"Data preprocessing completed."
    except Exception as e:
        return None, f"Error in preprocessing: {str(e)}"

@st.cache_resource
def load_models():
    """
    Load all saved models from the models directory
    """
    # Change path to models folder
    models_dir = "models"
    
    # Check if models directory exists
    if not os.path.exists(models_dir):
        return {}, ["âŒ Models directory not found. Please create a 'models' folder with model files."]
    
    # List model files from the models directory
    model_files = [f for f in os.listdir(models_dir) if f.endswith('_model.pkl')]
    models = {}
    messages = []
    
    for model_file in model_files:
        model_name = model_file.replace('_model.pkl', '').replace('_', ' ')
        try:
            # Load model from the models directory
            model_path = os.path.join(models_dir, model_file)
            model = joblib.load(model_path)
            models[model_name] = model
            messages.append(f"âœ… Loaded model: {model_name}")
        except Exception as e:
            messages.append(f"âŒ Error loading {model_file}: {e}")
    
    return models, messages

def predict_wqi(models, features_df, selected_models=None):
    """
    Predict WQI using all or selected models for new data
    """
    predictions = {}
    
    # Filter models if specific ones are selected
    if selected_models and len(selected_models) > 0:
        models_to_use = {name: model for name, model in models.items() if name in selected_models}
    else:
        models_to_use = models
    
    for name, model in models_to_use.items():
        try:
            pred = model.predict(features_df)
            predictions[name] = pred
        except Exception as e:
            st.error(f"Error predicting with {name}: {e}")
    
    return predictions

def get_download_link(df, filename, text):
    """Generate a download link for a dataframe"""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">ðŸ“¥ {text}</a>'
    return href

def manual_input_tab(models):
    """
    Create a tab for manual water quality parameter input
    """
    st.markdown("<h2 class='sub-header'>Manual WQI Parameter Input</h2>", unsafe_allow_html=True)
    
    # Define the columns to input
    columns_to_input = [
        'Alkalinity-total (as CaCO3)',
        'Ammonia-Total (as N)',
        'BOD - 5 days (Total)',
        'Chloride',
        'Conductivity @25Â°C',
        'Dissolved Oxygen',
        'ortho-Phosphate (as P) - unspecified',
        'pH',
        'Temperature',
        'Total Hardness (as CaCO3)',
        'True Colour'
    ]
    
    # Create input columns for better layout
    cols = st.columns(2)
    
    # Prepare a dictionary to store input values
    input_data = {}
    
    # Create input fields
    for i, column in enumerate(columns_to_input):
        col_index = i % 2  # Alternate between first and second column
        with cols[col_index]:
            # Add input field with appropriate type and help text
            if column == 'pH':
                input_data[column] = st.number_input(
                    column, 
                    min_value=0.0, 
                    max_value=14.0, 
                    value=7.0, 
                    step=0.1, 
                    help="pH ranges from 0 (acidic) to 14 (alkaline), with 7 being neutral"
                )
            else:
                input_data[column] = st.number_input(
                    column, 
                    min_value=0.0, 
                    value=0.0, 
                    step=0.1, 
                    help=f"Enter the value for {column}"
                )
    
    # Prediction button
    if st.button("Predict WQI from Manual Input", type="primary"):
        # Convert input to DataFrame
        input_df = pd.DataFrame([input_data])
        
        # Preprocess the input data (use the same preprocessing as before)
        with st.spinner("Preprocessing input data..."):
            processed_input, preprocess_msg = preprocess_data(input_df)
        
        if processed_input is None:
            st.error(preprocess_msg)
            return
        
        # Make predictions
        with st.spinner("Generating predictions..."):
            predictions = predict_wqi(models, processed_input)
            
            # Display predictions
            st.markdown("### Predictions")
            pred_results = pd.DataFrame(predictions)
            st.dataframe(pred_results, use_container_width=True)
            
            # Detailed view of input and predictions
            combined_results = pd.concat([input_df, pred_results], axis=1)
            
            # Download option
            st.markdown("### Download Results")
            st.markdown(get_download_link(combined_results, "manual_wqi_prediction.csv", 
                                        "Download Prediction CSV"), unsafe_allow_html=True)

def dataset_prediction_tab(data, models):
    """
    Create a tab for dataset-based WQI prediction
    """
    # Display raw data sample
    with st.expander("Preview Raw Data", expanded=False):
        st.dataframe(data.head(), use_container_width=True)

    # Check if WQI Value column exists
    has_wqi = 'WQI Value' in data.columns

    # Preprocess data
    with st.spinner("Preprocessing data..."):
        processed_data, preprocess_msg = preprocess_data(data)
    
    if processed_data is None:
        st.error(preprocess_msg)
        return

    # Display preprocessed data sample
    with st.expander("Preview Preprocessed Data", expanded=False):
        st.dataframe(processed_data.head(), use_container_width=True)

    # Prepare features
    if has_wqi:
        X_new = processed_data.drop(columns=['WQI Value'])
        y_new = processed_data['WQI Value']
    else:
        X_new = processed_data

    # Select models to use
    st.write("### Select models for prediction")
    selected_models = []

    # Use checkboxes for each model
    for model_name in models.keys():
        if st.checkbox(model_name, value=True):  # Default checked
            selected_models.append(model_name)

    # Make predictions button
    if st.button("Generate Predictions", type="primary"):
        if not selected_models:
            st.warning("Please select at least one model for prediction")
        else:
            with st.spinner("Generating predictions..."):
                # Make predictions
                predictions = predict_wqi(models, X_new, selected_models)
                
                # Create results DataFrame
                results = pd.DataFrame()
                
                if has_wqi:
                    results['Original WQI'] = y_new.reset_index(drop=True)
                
                for name, preds in predictions.items():
                    results[f'{name} Prediction'] = preds
                    
                    if has_wqi:
                        results[f'{name} Error'] = abs(results['Original WQI'] - preds)
                
            # Display predictions table
            st.markdown("<h3 class='sub-header'>Predictions</h3>", unsafe_allow_html=True)
            st.dataframe(results.head(20), use_container_width=True)
            
            # If we have original WQI values, display metrics
            if has_wqi:
                metrics = []
                for name, preds in predictions.items():
                    mse = mean_squared_error(y_new, preds)
                    r2 = r2_score(y_new, preds)
                    rmse = np.sqrt(mse)
                    metrics.append({
                        'Model': name,
                        'MSE': mse,
                        'RÂ²': r2,
                        'RMSE': rmse
                    })
                
                metrics_df = pd.DataFrame(metrics).sort_values('RÂ²', ascending=False)
                best_model = metrics_df.iloc[0]['Model']
                
                # Display metrics
                st.markdown("<h3 class='sub-header'>Performance Metrics</h3>", unsafe_allow_html=True)
                st.dataframe(metrics_df.style.highlight_max(subset=['RÂ²']).highlight_min(subset=['MSE', 'RMSE']), 
                            use_container_width=True)
                
                # Create visualization
                fig, axes = plt.subplots(1, 2, figsize=(12, 5))
                
                # Scatter plot for best model
                axes[0].scatter(results['Original WQI'], results[f'{best_model} Prediction'], alpha=0.5, color='blue')
                min_val = min(results['Original WQI'].min(), results[f'{best_model} Prediction'].min())
                max_val = max(results['Original WQI'].max(), results[f'{best_model} Prediction'].max())
                axes[0].plot([min_val, max_val], [min_val, max_val], 'r--')
                axes[0].set_title(f'Best Model ({best_model}): Predicted vs Actual')
                axes[0].set_xlabel('Actual WQI')
                axes[0].set_ylabel('Predicted WQI')
                
                # Error distribution
                sns.histplot(results[f'{best_model} Error'], kde=True, ax=axes[1], color='green')
                axes[1].set_title(f'Error Distribution for {best_model}')
                axes[1].set_xlabel('Absolute Error')
                
                plt.tight_layout()
                st.pyplot(fig)
            
            # Download predictions
            st.markdown("### Download Results")
            st.markdown(get_download_link(results, "wqi_predictions.csv", 
                                        "Download Predictions CSV"), unsafe_allow_html=True)
            
            # Download joined data with predictions
            full_results = pd.concat([data.reset_index(drop=True), 
                                    results.drop('Original WQI', errors='ignore')], axis=1)
            st.markdown(get_download_link(full_results, "full_predictions_with_data.csv", 
                                        "Download Full Results (Original Data + Predictions)"), unsafe_allow_html=True)

def main():
    # Header
    st.markdown("<h1 class='main-header'>Water Quality Index (WQI) Prediction</h1>", unsafe_allow_html=True)
    st.markdown("---")

    # Sidebar
    st.sidebar.image("https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExYzhkdmpvcWswcHVwMnlrM25pYnIxaDZsMmpiYmxrYXBwbGk1M3BwNCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Jt5kX3vh7JKeVGpidQ/giphy.gif", width=100)
    st.sidebar.title("WQI Prediction Tools")
    st.sidebar.markdown("---")

    # Define default dataset path
    DEFAULT_DATASET_PATH = "Dataset.csv"

    # Auto-load data when app starts
    data, message = load_data_from_local(DEFAULT_DATASET_PATH)

    if data is not None:
        st.session_state.data = data
        st.sidebar.success("Dataset Loaded Successfully")
    else:
        st.sidebar.error(message)

    # Load models
    models, model_messages = load_models()

    if not models:
        st.error("No models found. Make sure model files (.pkl) are in the 'models' directory.")
        st.stop()

    # Show model loading status
    with st.sidebar.expander("Model Loading Status", expanded=False):
        for msg in model_messages:
            st.write(msg)

    # Check if data is loaded
    if "data" not in st.session_state or st.session_state.data is None:
        st.error(f"Could not load the dataset from {DEFAULT_DATASET_PATH}. Please make sure the file exists and is accessible.")
        st.stop()

    # Create tabs
    tab1, tab2 = st.tabs(["Predict from Dataset", "Manual Input"])
    
    with tab1:
        dataset_prediction_tab(st.session_state.data, models)
    
    with tab2:
        manual_input_tab(models)

    # Add footer
    st.sidebar.markdown("---")
    st.sidebar.info("ðŸ’§ WQI Prediction App â€¢ Made from Danish")

# Run the main function
if __name__ == "__main__":
    main()
